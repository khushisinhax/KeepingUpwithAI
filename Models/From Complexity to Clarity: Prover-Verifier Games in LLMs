Large Language Models have revolutionized the field of natural language processing by providing powerful tools for understanding and generating human language
However, despite their impressive capabilities, understanding the reasoning behind their outputs has remained a significant challenge. 

To address this issue, researchers at OpenAI have introduced a novel approach called Prover-Verifier Games. This innovative method aims to enhance the legibility and trustworthiness of LLM outputs.

At the core of Prover-Verifier Games lies a collaborative process between two AI models: the Prover and the Verifier. 
The Prover - generates solutions to problems. 
The Verifier - assesses the Proverâ€™s work for correctness and clarity.

In laymen terms : 
Imagine a really smart computer. This computer is trying to be helpful and give you answers to questions. But sometimes, its answers are hard to understand.
So, we create another computer that's good at checking if the first computer's answers make sense.

The game is:

The smart computer tries to give answers that are easy to understand.
The checking computer checks if the answers are correct and easy to follow.
If the checking computer says something is wrong, the smart computer tries to explain it better. They keep doing this until the checking computer agrees that everything is correct and easy to understand.


To ensure the checking computer is correct :
1. Teach the checking computer with lots of examples of right and wrong answers.
2. Use of human experts

Why not teach the original computer to be both smart and capable of accurately checking its own work? - Bias! It would be very t's very difficult for a computer to be completely unbiased when checking its own work. 
It's like asking a student to grade their own test - they might be tempted to give themselves a better score.


Essentially, the PVG framework involves 2 kinds of training:

Checkability Training
Checkability training focuses on ensuring the Prover generates outputs that can be easily verified by the Verifier. This is achieved through a reinforcement learning process where:
The Prover generates an output.
The Verifier attempts to verify the output.
If the Verifier successfully verifies the output, the Prover is rewarded.
If the Verifier fails to verify the output, the Prover is penalized.
Over time, the Prover learns to produce outputs that are more likely to be verifiable, as this leads to a higher reward.

Legibility Training
Legibility training aims to improve the human-understandability of the Prover's outputs. This is often done in combination with checkability training.
Human evaluation: Human experts assess the legibility of the Prover's outputs.
Reward/penalty: The Prover is rewarded for outputs deemed highly legible and penalized for those that are difficult to understand.
By incorporating human feedback into the training process, the Prover learns to generate outputs that are both verifiable and easily comprehensible to humans.

It's important to note that checkability and legibility are often correlated. Outputs that are easily verifiable are often also more legible, as clarity is essential for verification. 
However, there might be cases where an output is verifiable but not necessarily legible to a human. Therefore, both aspects need to be considered during training.

This comprehensive approach aims to make LLM outputs more understandable and trustworthy, thus potentially aiding in the alignment of highly capable models with human values and expectations.

Link to the paper - https://arxiv.org/abs/2407.13692
